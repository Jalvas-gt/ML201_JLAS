---
title: "Final.Project"
format: html
editor: visual
author: Jos√© L. Alvarado
---

## Loading the proper packages

```{r message=FALSE, warning=FALSE}
library(matrixStats)
library(caret)
library(umap)
library(ggplot2)
library(dplyr)
library(class)
library(rpart)
library(randomForest)
```

## Loading and understanding the data

We begin exploring and making sense of the data.

```{r}

data("pbmc_facs", package = "fastTopics") #The database

x <- as.matrix(pbmc_facs$counts) #creating a matrix of the predictors
y <- pbmc_facs$samples$celltype #creating a vector of the labels

class(x)
class(y)


dim(x) 

length(y)
levels(y)

100*(mean(x == 0)) #To determine the amount of "zeros"

```

We see the following:

a)  x is a matrix of 3774 cells & 16791 genes. This is a gene expression matrix.

b)  y is a vector of 3774 factors. The factors are 10 different cell types. Therefore, y labels each of the 3774 cells with one of the 10 cell types.

c)  95.73% of counts is "0". The 4.27% of non-zero data are UMI's (UMI = Unique Molecular Identifier).

## Now let's do some data visualization


```{r}

hist(colMeans(x), breaks=100, main="Histogram of Average Gene Expression per Cell", xlab="Average Expression")

# Calculate total counts per cell
total_counts <- rowSums(x)

# Prepare data for ggplot
data_for_plot <- data.frame(TotalCounts = total_counts, CellType = y)

# Create the boxplot
ggplot(data_for_plot, aes(x = CellType, y = TotalCounts, fill = CellType)) +
  geom_boxplot() +
  scale_fill_viridis_d() +
  theme_minimal() +
  ggtitle("Distribution of Total Gene Expression Counts by Cell Type") +
  xlab("Cell Type") +
  ylab("Total Gene Expression Counts") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


```

The distribution shows count data & a big amount of "zeroes" (we already know it's 95.73%) which suggests a "zero inflated model" (I will not do a goodness of fit test, but it would be the way to test this hypothesis). This is problematic, as this distribution accounts for two models: 1 model for the distribution of zeroes and 1 model for the rest of the data. Doing regression on this, therefore, would be problematic and other approaches could be preferred.

The boxplot shows that CD34+ cells has a median count significantly higher than the other cell types, with a high level of variability within this group. In contrast, CD14+ Monocyte cell type exhibits the lowest median counts and few outliers, and also shows less within variability.

## Data transformation

Given what we have seen, it makes sense to transform the data to obtain a more favorable distribution.

```{r}
nz <- x == 0

# Let's compute residuals (errors)
n_i <- rowSums(x)
m_j <- colSums(x)/sum(n_i)
m_ij <- outer(n_i, m_j, FUN = "*") 

# Pearson residuals
r <- (x - m_ij)/sqrt(m_ij)

r <- sign(x - m_ij)*sqrt(2*(x*log(x/m_ij) - (x - m_ij)))
r[nz] <- -sqrt(2*m_ij[nz])
hist(r[,10548], nc= 100)



index <- createDataPartition(y, p = 0.18, list = FALSE)
col_ind <- order(-colSds(r))[1:500]


x_test <- r[test_ind, col_ind]
y_test <-  pbmc_facs$samples$celltype[test_ind]

x <- r[-test_ind, col_ind]
y <- pbmc_facs$samples$celltype[-test_ind]
```

Plotting the residuals gives us another perspective. We now have a distribution that resembles normal.


Now, for computational efficiency, we'll do partition of the cores to run analysis in parallel.
```{r}
library(parallel)
library(doParallel)
nc <- detectCores()  - 1
cl <- makeCluster(nc) # convention to leave 1 core for OS
registerDoParallel(cl)
```
## Data partition

Our data matrix is of dimensions: 3774 cells & 16791 genes (3774 x 16791), meaning 63,369,234 features.
To train a model, we must do so only in a subset of the data (the training set) and then we test it in the remaining data (the test set). This as a validation analysis. The training of the model cannot be done with any of the test set data (this is known as data leakage) as we aim to predict in the test set with the model we built with the training data.

For computational power reasons, we create a training set that includes 18% of the totality of the data. We will work with the transformed data.

```{r}
pc <- prcomp(x, rank. = 200)
summary(pc) #We see that the first 200 PCs explain ~ 75% of the variance

fit_knn <- train(pc$x, y, method = "knn",
                 tuneGrid = data.frame(k = seq(1,7,2)),
                 trControl = trainControl("cv", number = 20, p = 0.95))

plot(fit_knn)
y_hat <- predict(fit_knn, newdata = sweep(x_test, 2, colMeans(x)) %*% pc$rotation)
mean(y_hat == y_test)
plot(fit_knn_pca$results)
confusionMatrix(y_test, y_hat)
```

The accuracy is around 70% (IC 0.35 - 0.93), not the best outcome.

Now, the plot. Every concept is distributed in rows:

a) k vs. Accuracy: Shows how the accuracy of the model changes with different values of k. There isn't a clear trend visible, suggesting that accuracy doesn't improve consistently with higher or lower k values.

b) k vs. Kappa: Kappa is another metric for classification performance that accounts for chance agreement. This plot again doesn't show a clear relationship between k and Kappa.

c) k vs. AccuracySD: This plots the standard deviation of the accuracy against k, which gives an idea about the variability of the model performance for different k values.

d) k vs. KappaSD: Similarly, this shows the variability in Kappa statistic for different values of k.

e) Accuracy vs. Kappa: Shows the relationship between accuracy and kappa, which seem to be positively correlated as expected since they are both measures of classification performance.

f) AccuracySD vs. KappaSD: Illustrates the relationship between the variability of accuracy and kappa. A lack of a clear pattern would suggest that the stability of the model performance in terms of accuracy does not necessarily correlate with the stability of the kappa statistic.

We can try different methodologies & compare.

## Random forest approach

We won't do it from scratch, we'll keep the normalized data
```{r}
fit_rf <- train(x, y, method = "rf",
                tuneGrid = data.frame(mtry = seq(5, 15, 3)),
                trControl = trainControl("cv", number = 10, p = 0.9))


plot(fit_rf)

fit_rf <- randomForest(x, y, mtry = 15)
y_hat <- predict(fit_rf, newdata = x_test)
mean(y_hat == y_test)

```

We now obtain an accuracy ~ 90%, much better that the one we obtained with the knn model.

## The function

Finally, now that we have selected our model, we create a function that mimics the process. Worth noting, the normalization of the data needs to be done ahead of running this function, but it woukd be propper to do so after doing exploratory data analysis.

```{r}

run_random_forest <- function(x, y, partition_size = 0.18, pca_thresh = 0.50, ntree = 150, cv_folds = 10) {
  # Load libraries
  library(caret)
  library(randomForest)
  
  # Partition the data
  set.seed(123) # for reproducibility
  index <- createDataPartition(y, p = partition_size, list = FALSE)
  x_train <- x[index, ]
  y_train <- factor(y[index])
  x_test <- x[-index, ]
  y_test <- factor(y[-index])
  
  # Preprocess the training set
  prep_rf <- preProcess(x_train, method = c("nzv", "pca"), thresh = pca_thresh)
  x_train_pca <- predict(prep_rf, x_train)
  
  # Train the Random Forest model
  train_cont <- trainControl(method = "cv", number = cv_folds)
  rf_model <- train(x_train_pca, y_train, method = "rf", trControl = train_cont, ntree = ntree)
  
  # Transform the test set
  x_test_pca <- predict(prep_rf, x_test)
  
  # Make predictions and evaluate the model
  predictions <- predict(rf_model, x_test_pca)
  conf_mat <- confusionMatrix(predictions, y_test)
  
  # Return the model, confusion matrix, and predictions
  list(
    model = rf_model,
    confusion_matrix = conf_mat,
    predictions = predictions
  )
}

# You would call the function like this:
results <- run_random_forest(x, y)

```
 
Other approach is to do it in two different functions. Let's see:

```{r}
# We'll create two functions. 

# Function 01: Model training function
train_random_forest <- function(x, y, partition_size = 0.18, pca_thresh = 0.50, ntree = 150, cv_folds = 10) {                                 #you can change the partition if considered neccesary
  library(caret)
  library(randomForest)

  # Partition the data
  set.seed(123)  # for reproducibility
  index <- createDataPartition(y, p = partition_size, list = FALSE)
  x_train <- x[index, ]
  y_train <- factor(y[index])
  x_test <- x[-index, ]
  y_test <- factor(y[-index])

  # Preprocess the training set
  prep_rf <- preProcess(x_train, method = c("nzv", "pca"), thresh = pca_thresh)
  x_train_pca <- predict(prep_rf, x_train)

  # Train the Random Forest model
  train_cont <- trainControl(method = "cv", number = cv_folds)
  rf_model <- train(x_train_pca, y_train, method = "rf", trControl = train_cont, ntree = ntree)

  # Save the trained model and preprocessing object
  saveRDS(rf_model, "rf_model.rds")
  saveRDS(prep_rf, "prep_rf.rds")

  # Transform the test set
  x_test_pca <- predict(prep_rf, x_test)

  # Make predictions and evaluate the model
  predictions <- predict(rf_model, x_test_pca)
  conf_mat <- confusionMatrix(predictions, y_test)

  # Return the model, the confusion matrix, and predictions
  list(
    model = rf_model,
    preprocessing = prep_rf,
    confusion_matrix = conf_mat,
    predictions = predictions
  )
}

#Function 02: the prediction function

predict_cell_type <- function(gene_expression_vector) {
  # Load the model and preprocessing object
  rf_model <- readRDS("rf_model.rds")
  prep_rf <- readRDS("prep_rf.rds")

  # Ensure the input is a matrix and apply the preprocessing
  gene_expression_matrix <- as.matrix(t(gene_expression_vector))
  gene_expression_pca <- predict(prep_rf, gene_expression_matrix)

  # Predict the cell type
  predicted_cell_type <- predict(rf_model, gene_expression_pca)

  return(predicted_cell_type)
}

#Now, the use of the function: (`x` and `y` are loaded and preprocessed)
training_results <- train_random_forest(x, y)
# Example gene expression vector. Make sure it matches trained genes
new_gene_expression <- c(gene1 = 123, gene2 = 456, gene3 = 789, ...)  # 

# Predict the cell type
predicted_type <- predict_cell_type(new_gene_expression)
print(predicted_type)


#It is worth noting, good understanding of the data set is a prerequisite.
```

So, what do these functions do?

These functions are designed to simplify the process of training a Random Forest model and using it to predict cell types from gene expression data.

Function 01: first step in the workflow. Responsible for building the predictive model. It begins by dividing the provided gene expression data (x) and the corresponding cell types (y) into two sets: one for training the model and the other for testing its accuracy. During training, it preprocesses the data to remove less informative features (nzv = "near zero variance") and reduces dimensionality through Principal Component Analysis (PCA), which simplifies the data while retaining essential information (variance). Then, it trains the Random Forest (RF) model, which is about creating multiple decision trees to make predictions. Then, the function evaluates the model's performance using the test data, looking into how well the model might perform on unseen data.

Function 02: now with the trained model, this function predicts the cell type of new gene expression samples. It takes a vector of gene expression values as input, applies the same preprocessing transformations as were used during the training (PCA), and then uses the trained RF model to predict the cell type.


```{r}
stopCluster(cl)
stopImplicitCluster()
```
